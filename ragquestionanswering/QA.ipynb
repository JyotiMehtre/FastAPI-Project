{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a87a20fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1c89338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD PDF\n",
    "\n",
    "loader = PyPDFLoader(r\"c:\\Users\\jyoti\\Downloads\\Grandma's Bag of Stories by Sudha Murthy.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "513feaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into chunks\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0b77592e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Embeddings + FAISS vector store\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "db = FAISS.from_documents(chunks, emb)\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00e8061b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM (Ollama)\n",
    "\n",
    "llm = Ollama(model=\"qwen2:0.5b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37960c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_context(inputs):\n",
    "    return f\"Use the following context to answer the question.\\n\\nContext:\\n{inputs['context']}\\n\\nQuestion: {inputs['question']}\\nAnswer:\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e50719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \n",
    "     \"question\": RunnablePassthrough()}\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "14e02463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.runnables.base.RunnableParallel<context,question>Input'>\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.input_schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c769d2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'RunnableParallel<context,question>Input', 'type': 'string'}\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.input_schema.schema())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e300405e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first={\n",
      "  context: VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002350D6AE390>, search_kwargs={})\n",
      "           | RunnableLambda(format_docs),\n",
      "  question: RunnablePassthrough()\n",
      "} middle=[Ollama(model='qwen2:0.5b')] last=StrOutputParser()\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "60c94707",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m response = \u001b[43mrag_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSummarize this PDF\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Generativeai25\\projects\\ragquestionanswering\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3090\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3088\u001b[39m                 input_ = context.run(step.invoke, input_, config, **kwargs)\n\u001b[32m   3089\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3090\u001b[39m                 input_ = context.run(step.invoke, input_, config)\n\u001b[32m   3091\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3092\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Generativeai25\\projects\\ragquestionanswering\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:374\u001b[39m, in \u001b[36mBaseLLM.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    362\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    364\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    369\u001b[39m     **kwargs: Any,\n\u001b[32m    370\u001b[39m ) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    371\u001b[39m     config = ensure_config(config)\n\u001b[32m    372\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    373\u001b[39m         \u001b[38;5;28mself\u001b[39m.generate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m374\u001b[39m             [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    375\u001b[39m             stop=stop,\n\u001b[32m    376\u001b[39m             callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    377\u001b[39m             tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    378\u001b[39m             metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    379\u001b[39m             run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    380\u001b[39m             run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    381\u001b[39m             **kwargs,\n\u001b[32m    382\u001b[39m         )\n\u001b[32m    383\u001b[39m         .generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]\n\u001b[32m    384\u001b[39m         .text\n\u001b[32m    385\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Generativeai25\\projects\\ragquestionanswering\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\llms.py:323\u001b[39m, in \u001b[36mBaseLLM._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=convert_to_messages(model_input))\n\u001b[32m    319\u001b[39m msg = (\n\u001b[32m    320\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: Invalid input type <class 'dict'>. Must be a PromptValue, str, or list of BaseMessages."
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Summarize this PDF\")\n",
    "print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragquestionanswering (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
